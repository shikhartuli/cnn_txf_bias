{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"vit_jax_fine-tune_imagenet.ipynb","provenance":[{"file_id":"1GrffHh-LzdY2HHok1RFxbsc33RnkNOHO","timestamp":1606560122624}],"collapsed_sections":["sXhZm0kpPpH6"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-MYot7DJh9kk"},"source":["See code at https://github.com/google-research/vision_transformer/\n","\n","See paper at https://arxiv.org/abs/2010.11929\n","\n","This Colab allows you to run the [JAX](https://jax.readthedocs.org) implementation of the Vision Transformer."]},{"cell_type":"markdown","metadata":{"id":"sXhZm0kpPpH6"},"source":["##### Copyright 2020 Google LLC."]},{"cell_type":"code","metadata":{"cellView":"form","id":"KfmzfvFxPuk7","executionInfo":{"status":"ok","timestamp":1610870902761,"user_tz":-330,"elapsed":3683,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}}},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iOVCm4CnP1Do"},"source":["<a href=\"https://colab.research.google.com/github/google-research/vision_transformer/blob/master/vit_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"cyD76dm5JaeW"},"source":["### Setup\n","\n","Needs to be executed once in every VM.\n","\n","The cell below downloads the code from Github and install necessary dependencies."]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"zZvI8OXt78sj","executionInfo":{"status":"ok","timestamp":1610870947255,"user_tz":-330,"elapsed":48157,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}},"outputId":"5f3b024a-d80e-41a3-8103-7a8b290ac9ff"},"source":["#@markdown Select whether you would like to store data in your personal drive.\n","#@markdown\n","#@markdown If you select **yes**, you will need to authorize Colab to access\n","#@markdown your personal drive\n","#@markdown\n","#@markdown If you select **no**, then any changes you make will diappear when\n","#@markdown this Colab's VM restarts after some time of inactivity...\n","use_gdrive = 'yes'  #@param [\"yes\", \"no\"]\n","account = \"shikhartuli98@gmail.com\" #@param [\"shikhartuli98@gmail.com\", \"stuli@princeton.edu\"]\n","\n","if use_gdrive == 'yes':\n","  from google.colab import drive\n","  drive.mount('/gdrive')\n","  if account == \"shikhartuli98@gmail.com\":\n","    root = '/gdrive/My Drive/cnn_txf_bias/vision_transformer'\n","  else:\n","    root = '/gdrive/My Drive/Fall 20-21/COS 454/Project/cnn_txf_bias/vision_transformer'\n","  import os\n","  if not os.path.isdir(root):\n","    os.mkdir(root)\n","  os.chdir(root)\n","  print(f'\\nChanged CWD to \"{root}\"')\n","else:\n","  from IPython import display\n","  display.display(display.HTML(\n","      '<h1 style=\"color:red\">CHANGES NOT PERSISTED</h1>'))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n","\n","Changed CWD to \"/gdrive/My Drive/cnn_txf_bias/vision_transformer\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sCN4d-GQJdU4","executionInfo":{"status":"ok","timestamp":1610870995910,"user_tz":-330,"elapsed":96802,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}},"outputId":"a36df915-f0e3-4c6c-b958-58b1b8a17153"},"source":["!pip install -qr ./vit_jax/requirements_new.txt\n","!pip install tfa-nightly\n","!pip install tensorflow_io\n","!pip install tfds-nightly"],"execution_count":3,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K     |█████▍                          | 10kB 20.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 20kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 30kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 40kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 51kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.7MB/s \n","\u001b[K     |████████████████████████████████| 153kB 7.3MB/s \n","\u001b[K     |████████████████████████████████| 92kB 5.8MB/s \n","\u001b[K     |████████████████████████████████| 144.5MB 80kB/s \n","\u001b[K     |████████████████████████████████| 4.3MB 56.4MB/s \n","\u001b[?25hCollecting tfa-nightly\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/5d/6669589cb5068f6007a55a10f7ba92bebb8702f24fc5f7d5aae799966df5/tfa_nightly-0.13.0.dev20210115223011-cp36-cp36m-manylinux2010_x86_64.whl (706kB)\n","\u001b[K     |████████████████████████████████| 716kB 5.3MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.6/dist-packages (from tfa-nightly) (2.7.1)\n","Installing collected packages: tfa-nightly\n","Successfully installed tfa-nightly-0.13.0.dev20210115223011\n","Collecting tensorflow_io\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/3c/b45c30448cd6a04f25b088da024229149323fa44bc6322a7372bb556eada/tensorflow_io-0.17.0-cp36-cp36m-manylinux2010_x86_64.whl (25.3MB)\n","\u001b[K     |████████████████████████████████| 25.3MB 1.5MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_io) (2.4.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.6.3)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.12.4)\n","Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.4.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.12)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.12.1)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.15.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.36.2)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.1.2)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.3.3)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.19.5)\n","Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.4.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.3.0)\n","Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.32.0)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.2.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.7.4.3)\n","Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.10.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.1.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.10.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (51.1.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.3.3)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.17.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.4.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.7.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.23.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.3.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (4.2.0)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (4.6)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.3.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2020.12.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.4.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.1.0)\n","Installing collected packages: tensorflow-io\n","Successfully installed tensorflow-io-0.17.0\n","Collecting tfds-nightly\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/8c/e8edca02e927b3994cf17e5ca8d353258057c95ed33194eb0fa5b60548d5/tfds_nightly-4.2.0.dev202101160107-py3-none-any.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 5.5MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (3.12.4)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.1.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.16.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.10.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.3.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.15.0)\n","Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (3.7.4.3)\n","Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.3)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.8)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.26.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.19.5)\n","Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (20.3.0)\n","Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (4.1.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (4.41.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.2->tfds-nightly) (51.1.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tfds-nightly) (1.52.0)\n","Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly) (3.4.0)\n","Installing collected packages: tfds-nightly\n","Successfully installed tfds-nightly-4.2.0.dev202101160107\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"4EzOChfJeVrU","executionInfo":{"status":"ok","timestamp":1610871018472,"user_tz":-330,"elapsed":119358,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}},"outputId":"22bbdef0-117f-4fc5-cfe2-a0fd571c6d73"},"source":["#@markdown TPU setup : Boilerplate for connecting JAX to TPU.\n","\n","import os\n","if 'google.colab' in str(get_ipython()) and 'COLAB_TPU_ADDR' in os.environ:\n","  # Make sure the Colab Runtime is set to Accelerator: TPU.\n","  import requests\n","  if 'TPU_DRIVER_MODE' not in globals():\n","    url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20191206'\n","    resp = requests.post(url)\n","    TPU_DRIVER_MODE = 1\n","\n","  # The following is required to use TPU Driver as JAX's backend.\n","  from jax.config import config\n","  config.FLAGS.jax_xla_backend = \"tpu_driver\"\n","  config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n","  print('Registered TPU:', config.FLAGS.jax_backend_target)\n","else:\n","  print('No TPU detected. Can be changed under \"Runtime/Change runtime type\".')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Registered TPU: grpc://10.62.38.178:8470\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bcLBTSXuNjK6"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"6ztOhq_fzZyO","executionInfo":{"status":"ok","timestamp":1610871018473,"user_tz":-330,"elapsed":119353,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}}},"source":["# Specify model.\n","model = 'ViT-B_32'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"igqZ6qYNeHWo","executionInfo":{"status":"ok","timestamp":1610871048098,"user_tz":-330,"elapsed":148972,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}},"outputId":"529b0ba8-229a-454e-ac9b-7b8ea2c396d6"},"source":["import flax\n","import jax\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import tqdm\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","import flax.jax_utils as flax_utils\n","import random\n","import cv2\n","import csv\n","import re\n","import os\n","import tensorflow_io as tfio\n","\n","# Shows the number of available devices.\n","# In a CPU/GPU runtime this will be a single device.\n","# In a TPU runtime this will be 8 cores.\n","jax.local_devices()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[TpuDevice(id=0, host_id=0, coords=(0,0,0), core_on_chip=0),\n"," TpuDevice(id=1, host_id=0, coords=(0,0,0), core_on_chip=0),\n"," TpuDevice(id=2, host_id=0, coords=(0,0,0), core_on_chip=0),\n"," TpuDevice(id=3, host_id=0, coords=(0,0,0), core_on_chip=0),\n"," TpuDevice(id=4, host_id=0, coords=(0,0,0), core_on_chip=0),\n"," TpuDevice(id=5, host_id=0, coords=(0,0,0), core_on_chip=0),\n"," TpuDevice(id=6, host_id=0, coords=(0,0,0), core_on_chip=0),\n"," TpuDevice(id=7, host_id=0, coords=(0,0,0), core_on_chip=0)]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"sjN0_b-YbaHu","executionInfo":{"status":"ok","timestamp":1610871052233,"user_tz":-330,"elapsed":153096,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}}},"source":["# Import files from repository.\n","# Updating the files in the editor on the right will immediately update the\n","# modules by re-importing them.\n","\n","import sys\n","if './vision_transformer' not in sys.path:\n","  sys.path.append('./vision_transformer')\n","\n","# Adding simclr directory to path\n","sys.path.append('../simclr/')\n","\n","%load_ext autoreload\n","%autoreload 2\n","%reload_ext autoreload\n","\n","from vit_jax import checkpoint\n","from vit_jax import hyper\n","from vit_jax import input_pipeline\n","from vit_jax import logging\n","from vit_jax import models\n","from vit_jax import momentum_clip\n","from vit_jax import train\n","\n","import gc\n","\n","# Importing data-augmentation functions\n","import data_util\n","\n","logger = logging.setup_logger('./logs')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"GojydzsXgknd","executionInfo":{"status":"ok","timestamp":1610871052567,"user_tz":-330,"elapsed":153423,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}}},"source":["# Helper functions for images.\n","\n","labelnames = dict(\n","  # https://www.cs.toronto.edu/~kriz/cifar.html\n","  cifar10=('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'),\n","  # https://www.cs.toronto.edu/~kriz/cifar.html\n","  cifar100=('apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'computer_keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'),\n","  imagenet2012=tuple(open('ilsvrc2012_wordnet_lemmas.txt'))\n",")\n","def make_label_getter(dataset):\n","  \"\"\"Returns a function converting label indices to names.\"\"\"\n","  def getter(label):\n","    if dataset in labelnames:\n","      return labelnames[dataset][label]\n","    return f'label={label}'\n","  return getter\n","\n","def show_img(img, ax=None, title=None):\n","  \"\"\"Shows a single image.\"\"\"\n","  if ax is None:\n","    ax = plt.gca()\n","  ax.imshow(img[...])\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  if title:\n","    ax.set_title(title)\n","\n","def show_img_grid(imgs, titles):\n","  \"\"\"Shows a grid of images.\"\"\"\n","  n = int(np.ceil(len(imgs)**.5))\n","  _, axs = plt.subplots(n, n, figsize=(3 * n, 3 * n))\n","  for i, (img, title) in enumerate(zip(imgs, titles)):\n","    img = (img + 1) / 2  # Denormalize\n","    show_img(img, axs[i // n][i % n], title)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QZfK1vIIMmFz"},"source":["### Load dataset"]},{"cell_type":"code","metadata":{"id":"TSAVpYtP5VaE","executionInfo":{"status":"ok","timestamp":1610871052568,"user_tz":-330,"elapsed":153417,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}}},"source":["dataset = 'imagenet2012'\n","batch_size = 512  # Reduce to 256 if running on a single GPU."],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SaiRhJYjUxZC","executionInfo":{"status":"ok","timestamp":1610871052932,"user_tz":-330,"elapsed":153772,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}},"outputId":"3d7ec964-0666-4b82-bde1-291c97d48f08"},"source":["input_pipeline.DEBUG"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ruzdzpsMNhGm","executionInfo":{"status":"ok","timestamp":1610871076836,"user_tz":-330,"elapsed":177665,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}},"outputId":"fa99af06-ff33-4fad-f19a-58769ea20aec"},"source":["# Note the datasets are configured in input_pipeline.DATASET_PRESETS\n","# Have a look in the editor at the right.\n","num_classes = 1000 # input_pipeline.get_dataset_info(dataset, 'train')['num_classes']\n","    # tf.data.Datset for training, infinite repeats.\n","ds_train = input_pipeline.get_data(\n","    dataset=dataset, mode='train', repeats=None, batch_size=batch_size, inception_crop=False,\n","    # tfds_manual_dir = '/scratch/network/stuli/datasets/imagenet2012',\n","    tfds_data_dir = './tensorflow_datasets'\n",")\n","    # tf.data.Datset for evaluation, single repeat.\n","ds_test = input_pipeline.get_data(\n","    dataset=dataset, mode='test', repeats=1, batch_size=batch_size, \n","    # tfds_manual_dir = '/scratch/network/stuli/datasets/imagenet2012',\n","    tfds_data_dir = './tensorflow_datasets'\n",")"],"execution_count":11,"outputs":[{"output_type":"stream","text":["2021-01-17 08:10:51,089 [INFO] absl: Load dataset info from ./tensorflow_datasets/imagenet2012/5.1.0\n","2021-01-17 08:11:13,452 [INFO] absl: Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: imagenet2012/5.1.0\n","2021-01-17 08:11:13,794 [INFO] absl: Load dataset info from /tmp/tmpn9koagrztfds\n","2021-01-17 08:11:13,806 [INFO] absl: Field info.description from disk and from code do not match. Keeping the one from code.\n","2021-01-17 08:11:13,809 [INFO] absl: Field info.module_name from disk and from code do not match. Keeping the one from code.\n","2021-01-17 08:11:13,813 [INFO] absl: Reusing dataset imagenet2012 (./tensorflow_datasets/imagenet2012/5.1.0)\n","2021-01-17 08:11:13,814 [INFO] absl: Constructing tf.data.Dataset for split train[:], from ./tensorflow_datasets/imagenet2012/5.1.0\n","2021-01-17 08:11:14,726 [INFO] absl: Load dataset info from ./tensorflow_datasets/imagenet2012/5.1.0\n","2021-01-17 08:11:14,768 [INFO] absl: Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: imagenet2012/5.1.0\n","2021-01-17 08:11:14,935 [INFO] absl: Load dataset info from /tmp/tmpd_qs3nhqtfds\n","2021-01-17 08:11:14,946 [INFO] absl: Field info.description from disk and from code do not match. Keeping the one from code.\n","2021-01-17 08:11:14,950 [INFO] absl: Field info.module_name from disk and from code do not match. Keeping the one from code.\n","2021-01-17 08:11:14,954 [INFO] absl: Reusing dataset imagenet2012 (./tensorflow_datasets/imagenet2012/5.1.0)\n","2021-01-17 08:11:14,956 [INFO] absl: Constructing tf.data.Dataset for split validation[:], from ./tensorflow_datasets/imagenet2012/5.1.0\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"7c-LfxOJdj8_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610871124401,"user_tz":-330,"elapsed":225221,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}},"outputId":"7a01cfbb-8769-4f7e-b1e1-714cdb1df23e"},"source":["# Fetch a batch of test images for illustration purposes.\n","batch = next(iter(ds_test.as_numpy_iterator()))\n","# Note the shape : [num_local_devices, local_batch_size, h, w, c]\n","batch['image'].shape"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 64, 384, 384, 3)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"ehzbRTSN20E5"},"source":["### Load pre-trained"]},{"cell_type":"code","metadata":{"id":"DMKr-4nK3DlT","executionInfo":{"status":"ok","timestamp":1610871174483,"user_tz":-330,"elapsed":275294,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}}},"source":["# Load model definition & initialize random parameters.\n","VisionTransformer = models.KNOWN_MODELS[model].partial(num_classes=num_classes)\n","_, params = VisionTransformer.init_by_shape(\n","    jax.random.PRNGKey(0),\n","    # Discard the \"num_local_devices\" dimension of the batch for initialization.\n","    [(batch['image'].shape[1:], batch['image'].dtype.name)])\n","\n","models_done = len(os.listdir('./vit_models/imagenet21k+imagenet2012+imagenet2012/'))\n","\n","experiments = ['Baseline', 'Rotate', 'Cutout', 'Sobel Filtering', 'Gaussian Blur', 'Color Distortion', 'Gaussain Noise']\n","\n","if models_done > 1:\n","  exp_done = 'Baseline'\n","  for i in range(models_done-1):\n","    exp_done = f'{exp_done}+{experiments[i+1]}'\n","  params = checkpoint.load(f'./vit_models/imagenet21k+imagenet2012+imagenet2012/{model}_{exp_done}.npz')\n","  params['pre_logits'] = {}\n","else:\n","  params = checkpoint.load(f'./vit_models/imagenet21k+imagenet2012/{model}.npz')\n","  params['pre_logits'] = {}"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T0TR5ETAh2C3","executionInfo":{"status":"ok","timestamp":1610871174485,"user_tz":-330,"elapsed":275287,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}},"outputId":"2874e931-2818-45da-bac9-e509b3160fc0"},"source":["models_done"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"zIXjOEDkvAWM","executionInfo":{"status":"ok","timestamp":1610871174485,"user_tz":-330,"elapsed":275278,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}}},"source":["# Load and convert pretrained checkpoint.\n","# This involves loading the actual pre-trained model results, but then also also\n","# modifying the parameters a bit, e.g. changing the final layers, and resizing\n","# the positional embeddings.\n","# For details, refer to the code and to the methods of the paper.\n","\n","# params = checkpoint.load(f'./vit_models/imagenet21k+imagenet2012/{model}.npz')\n","# params['pre_logits'] = {} \n","\n","# params = checkpoint.load_pretrained(\n","#     pretrained_path=f'./vit_models/imagenet21k+imagenet2012/{model}.npz',\n","#     init_params=params,\n","#     model_config=models.CONFIGS[model],\n","#     logger=logger,\n","# )"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aQVKzhaR8o-J"},"source":["### Evaluate"]},{"cell_type":"code","metadata":{"id":"WB6ywRTY-LOa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610871174866,"user_tz":-330,"elapsed":275648,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}},"outputId":"b6fed7f8-39ad-4539-ad82-b7f8afece756"},"source":["# So far, all our data is in the host memory. Let's now replicate the arrays\n","# into the devices.\n","# This will make every array in the pytree params become a ShardedDeviceArray\n","# that has the same data replicated across all local devices.\n","# For TPU it replicates the params in every core.\n","# For a single GPU this simply moves the data onto the device.\n","# For CPU it simply creates a copy.\n","params_repl = flax.jax_utils.replicate(params)\n","print('params.cls:', type(params['cls']).__name__, params['cls'].shape)\n","print('params_repl.cls:', type(params_repl['cls']).__name__, params_repl['cls'].shape)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["params.cls: ndarray (1, 1, 768)\n","params_repl.cls: ShardedDeviceArray (8, 1, 1, 768)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_unNxEZAK0Cu","executionInfo":{"status":"ok","timestamp":1610871174867,"user_tz":-330,"elapsed":275640,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}}},"source":["# Then map the call to our model's forward pass onto all available devices.\n","vit_apply_repl = jax.pmap(VisionTransformer.call)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZgjFBUQ88p4z","executionInfo":{"status":"ok","timestamp":1610871174868,"user_tz":-330,"elapsed":275635,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}}},"source":["def get_accuracy(params_repl):\n","  \"\"\"Returns accuracy evaluated on the test set.\"\"\"\n","  good = total = 0\n","  steps = input_pipeline.get_dataset_info(dataset, 'test')['num_examples'] // batch_size\n","  for _, batch in zip(tqdm.notebook.trange(steps), ds_test.as_numpy_iterator()):\n","    predicted = vit_apply_repl(params_repl, batch['image'])\n","    is_same = predicted.argmax(axis=-1) == batch['label'].argmax(axis=-1)\n","    good += is_same.sum()\n","    total += len(is_same.flatten())\n","  return good / total"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"3qc7j0lv-F6-","executionInfo":{"status":"ok","timestamp":1610871174868,"user_tz":-330,"elapsed":275629,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}}},"source":["# Random performance without fine-tuning.\n","# get_accuracy(params_repl)"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HxMdU_e5NeoT"},"source":["### Fine-tune"]},{"cell_type":"code","metadata":{"id":"MI62dexw8mGo","executionInfo":{"status":"ok","timestamp":1610871175196,"user_tz":-330,"elapsed":275952,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}}},"source":["# 100 Steps take approximately 15 minutes in the TPU runtime.\n","total_steps = 50\n","warmup_steps = 5\n","decay_type = 'cosine'\n","grad_norm_clip = 1\n","# This controls in how many forward passes the batch is split. 8 works well with\n","# a TPU runtime that has 8 devices. 64 should work on a GPU. You can of course\n","# also adjust the batch_size above, but that would require you to adjust the\n","# learning rate accordingly.\n","accum_steps = 8\n","base_lr = 0.03"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"vzlfREb1ZHBY","executionInfo":{"status":"ok","timestamp":1610871183227,"user_tz":-330,"elapsed":283978,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}}},"source":["# Check out train.make_update_fn in the editor on the right side for details.\n","update_fn_repl = train.make_update_fn(VisionTransformer.call, accum_steps)\n","# We use a momentum optimizer that uses half precision for state to save\n","# memory. It als implements the gradient clipping.\n","opt = momentum_clip.Optimizer(grad_norm_clip=grad_norm_clip).create(params)\n","opt_repl = flax.jax_utils.replicate(opt)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"RTU7OmgjHb-G","executionInfo":{"status":"ok","timestamp":1610871186493,"user_tz":-330,"elapsed":287239,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}}},"source":["lr_fn = hyper.create_learning_rate_schedule(total_steps, base_lr, decay_type, warmup_steps)\n","# Prefetch entire learning rate schedule onto devices. Otherwise we would have\n","# a slow transfer from host to devices in every step.\n","lr_iter = hyper.lr_prefetch_iter(lr_fn, 0, total_steps)\n","# Initialize PRNGs for dropout.\n","update_rngs = jax.random.split(jax.random.PRNGKey(0), jax.local_device_count())"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"hwjsOz1bUWn_","executionInfo":{"status":"ok","timestamp":1610871186493,"user_tz":-330,"elapsed":287235,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}}},"source":["airplane_indices = [404]\n","bear_indices = [294, 295, 296, 297]\n","bicycle_indices = [444, 671]\n","bird_indices = [8, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 22, 23,\n","                24, 80, 81, 82, 83, 87, 88, 89, 90, 91, 92, 93,\n","                94, 95, 96, 98, 99, 100, 127, 128, 129, 130, 131,\n","                132, 133, 135, 136, 137, 138, 139, 140, 141, 142,\n","                143, 144, 145]\n","boat_indices = [472, 554, 625, 814, 914]\n","bottle_indices = [440, 720, 737, 898, 899, 901, 907]\n","car_indices = [436, 511, 817]\n","cat_indices = [281, 282, 283, 284, 285, 286]\n","chair_indices = [423, 559, 765, 857]\n","clock_indices = [409, 530, 892]\n","dog_indices = [152, 153, 154, 155, 156, 157, 158, 159, 160, 161,\n","                162, 163, 164, 165, 166, 167, 168, 169, 170, 171,\n","                172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n","                182, 183, 184, 185, 186, 187, 188, 189, 190, 191,\n","                193, 194, 195, 196, 197, 198, 199, 200, 201, 202,\n","                203, 205, 206, 207, 208, 209, 210, 211, 212, 213,\n","                214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n","                224, 225, 226, 228, 229, 230, 231, 232, 233, 234,\n","                235, 236, 237, 238, 239, 240, 241, 243, 244, 245,\n","                246, 247, 248, 249, 250, 252, 253, 254, 255, 256,\n","                257, 259, 261, 262, 263, 265, 266, 267, 268]\n","elephant_indices = [385, 386] \n","keyboard_indices = [508, 878]\n","knife_indices = [499]\n","oven_indices = [766]\n","truck_indices = [555, 569, 656, 675, 717, 734, 864, 867]\n","\n","category_indices = [airplane_indices, bear_indices, bicycle_indices, bird_indices, boat_indices,\n","                    bottle_indices, car_indices, cat_indices, chair_indices, clock_indices,\n","                    dog_indices, elephant_indices, keyboard_indices, knife_indices,\n","                    oven_indices, truck_indices]"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"uday2tsE_hTV"},"source":["# Training with data-augmentation\n","\n","tf.compat.v1.enable_eager_execution\n","\n","experiments = ['Baseline', 'Rotate', 'Cutout', 'Sobel Filtering', 'Gaussian Blur', 'Color Distortion', 'Gaussain Noise']\n","\n","# categories_cifar10 = ['airplane', 'automobile', 'bird', 'cat', 'dog', 'ship', 'truck']\n","# categories_SIN = ['airplane', 'car', 'bird', 'cat', 'dog', 'boat', 'truck']\n","\n","categories = os.listdir(\"./stimuli/cue-conflict/\")\n","categories.sort()\n","\n","acc_imagenet = []\n","shape_match = []\n","texture_match = []\n","shape_bias = []\n","\n","# Calculating total images in SIN-subset\n","total_images = 0\n","for c in categories:\n","  for im in os.listdir(\"./stimuli/cue-conflict/\" + c):\n","    shape_label = c\n","    texture_label = re.search('-(.*).png', im).group(1)[:-1]\n","    total_images += 1\n","\n","for exp in range(models_done - 1 if models_done > 1 else 0, len(experiments)):\n","# for exp in range(1, len(experiments)):\n","  print(f'Running Experiment: {experiments[exp]}')\n","\n","  lr_iter = hyper.lr_prefetch_iter(lr_fn, 0, total_steps)\n","  update_rngs = jax.random.split(jax.random.PRNGKey(0), jax.local_device_count())\n","\n","  # if exp == 0:\n","  #   # Training loop\n","  #   for step, batch, lr_repl in zip(tqdm.notebook.trange(1, total_steps + 1), ds_train.as_numpy_iterator(), lr_iter):\n","  #     opt_repl, loss_repl, update_rngs = update_fn_repl(opt_repl, lr_repl, batch, update_rngs)\n","\n","  if exp == 1:\n","    # Training loop\n","    for step, batch, lr_repl in zip(tqdm.notebook.trange(1, total_steps + 1), ds_train.as_numpy_iterator(), lr_iter):\n","      images, labels = batch['image'], batch['label']\n","      for i in range(images.shape[0]):\n","        bi = tf.image.rot90(images[i], random.randint(1, 3))\n","        images[i] = bi.numpy()\n","      batch_new = {'image': images, 'label': labels}\n","      opt_repl, loss_repl, update_rngs = update_fn_repl(opt_repl, lr_repl, batch_new, update_rngs)\n","\n","  if exp == 2:\n","    # Training loop\n","    for step, batch, lr_repl in zip(tqdm.notebook.trange(1, total_steps + 1), ds_train.as_numpy_iterator(), lr_iter):\n","      images, labels = batch['image'], batch['label']\n","      for i in range(images.shape[0]):\n","        bi = tfa.image.cutout_ops.random_cutout(images[i], tuple([np.int32(random.randrange(2, 192, 2)), np.int32(random.randrange(2, 192, 2))]))\n","        images[i] = bi.numpy()\n","      batch_new = {'image': images, 'label': labels}\n","      opt_repl, loss_repl, update_rngs = update_fn_repl(opt_repl, lr_repl, batch_new, update_rngs)\n","\n","  if exp == 3:\n","    # Training loop\n","    for step, batch, lr_repl in zip(tqdm.notebook.trange(1, total_steps + 1), ds_train.as_numpy_iterator(), lr_iter):\n","      images, labels = batch['image'], batch['label']\n","      for i in range(images.shape[0]):\n","        bi = tfio.experimental.filter.sobel(images[i])\n","        images[i] = bi.numpy()\n","      batch_new = {'image': images, 'label': labels}\n","      opt_repl, loss_repl, update_rngs = update_fn_repl(opt_repl, lr_repl, batch_new, update_rngs)\n","  \n","  if exp == 4:\n","    # Training loop\n","    for step, batch, lr_repl in zip(tqdm.notebook.trange(1, total_steps + 1), ds_train.as_numpy_iterator(), lr_iter):\n","      images, labels = batch['image'], batch['label']\n","      for i in range(images.shape[0]):\n","        bi = tfa.image.gaussian_filter2d(images[i]) #, filter_shape = [30, 30], sigma = 5)\n","        images[i] = bi.numpy()\n","      batch_new = {'image': images, 'label': labels}\n","      opt_repl, loss_repl, update_rngs = update_fn_repl(opt_repl, lr_repl, batch_new, update_rngs)\n","\n","  if exp == 5:\n","    # Training loop\n","    for step, batch, lr_repl in zip(tqdm.notebook.trange(1, total_steps + 1), ds_train.as_numpy_iterator(), lr_iter):\n","      images, labels = batch['image'], batch['label']\n","      for i in range(images.shape[0]):\n","        bi = data_util.random_color_jitter(images[i])\n","        images[i] = bi\n","      batch_new = {'image': images, 'label': labels}\n","      opt_repl, loss_repl, update_rngs = update_fn_repl(opt_repl, lr_repl, batch_new, update_rngs)\n","\n","  if exp == 6:\n","    # Training loop\n","    for step, batch, lr_repl in zip(tqdm.notebook.trange(1, total_steps + 1), ds_train.as_numpy_iterator(), lr_iter):\n","      images, labels = batch['image'], batch['label']\n","      for i in range(images.shape[0]):\n","        bi = tf.clip_by_value(images[i] + tf.random.normal(shape=tf.shape(images[i]), mean=0.0, stddev=(50)/(255), dtype=tf.float32), 0.0, 1.0)\n","        images[i] = bi.numpy()\n","      batch_new = {'image': images, 'label': labels}\n","      opt_repl, loss_repl, update_rngs = update_fn_repl(opt_repl, lr_repl, batch_new, update_rngs)\n","\n","  exp_done = 'Baseline'\n","  for i in range(exp):\n","    exp_done = f'{exp_done}+{experiments[i+1]}'\n","  \n","  # Save model\n","  checkpoint.save(flax_utils.unreplicate(opt_repl.target), f'./vit_models/imagenet21k+imagenet2012+imagenet2012/{model}_{exp_done}.npz')\n","\n","  # Accuracy on ImageNet\n","  print('Testing model on ImageNet')\n","  acc_imagenet.append(np.float(get_accuracy(opt_repl.target)))\n","  print(f'Accuracy = {acc_imagenet[-1]}')\n","\n","  VisionTransformer = models.KNOWN_MODELS[model].partial(num_classes=num_classes)\n","  _, params = VisionTransformer.init_by_shape(jax.random.PRNGKey(0), [((1, 384, 384, 3), batch['image'].dtype.name)])\n","  params = checkpoint.load(f'./vit_models/imagenet21k+imagenet2012+imagenet2012/{model}_{exp_done}.npz')\n","  params['pre_logits'] = {}\n","\n","  count = 0\n","\n","  # SIN Test\n","  print('Running SIN Test')\n","  SIN_count = 0.0\n","  SIN_correct_shape = 0.0\n","  SIN_correct_texture = 0.0\n","  for c in categories:\n","    for im in os.listdir(\"./stimuli/cue-conflict/\" + c + \"/\"):\n","\n","      shape_label = c\n","      texture_label = re.search('-(.*).png', im).group(1)[:-1]\n","\n","      img = cv2.imread(\"./stimuli/cue-conflict/\" + c + \"/\" + im)\n","      img = cv2.resize(img, (384, 384))\n","      inp = (np.array(img) / 128 - 1)[None, ...]\n","\n","      logits, = VisionTransformer.call(params, inp)\n","      preds = flax.nn.softmax(logits)\n","\n","      # pred = preds.argsort()[-1]\n","      # pred_label = labelnames[dataset][pred]\n","\n","      preds_16 = np.zeros(16)\n","      for idx in range(1000):\n","        for ci in range(len(category_indices)):\n","          if idx in category_indices[ci]:\n","            preds_16[ci] += preds[idx]\n","\n","      try:\n","        SIN_correct_shape += 1 if (categories[preds_16.argsort()[-1]] == shape_label) else 0\n","      except:\n","        pass\n","      try:\n","        SIN_correct_texture += 1 if (categories[preds_16.argsort()[-1]] == texture_label) else 0\n","      except:\n","        pass\n","\n","      SIN_count += 1\n","\n","      count += 1\n","      print('\\r %0.2f%%' % (count/(total_images)*100), end='')\n","  \n","  print()\n","  \n","  shape_match.append(SIN_correct_shape / SIN_count)\n","  texture_match.append(SIN_correct_texture / SIN_count)\n","  shape_bias.append(shape_match[-1] / (texture_match[-1] + shape_match[-1]))\n","\n","  with open(f'./results/fine-tune/imagenet2012/vit_fine-tune.csv', mode='a') as csv_file:\n","    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n","\n","    if exp == 0: csv_writer.writerow(['Augmentation', 'Shape Bias', 'Shape Match', 'Texture Match', 'ImageNet Accuracy'])\n","    aug = experiments[exp] if exp == 0 else f'+{experiments[exp]}'\n","    csv_writer.writerow([aug, f'{shape_bias[-1]*100}%', f'{shape_match[-1]*100}%', f'{texture_match[-1]*100}%', f'{acc_imagenet[-1]*100}%'])\n"],"execution_count":null,"outputs":[]}]}