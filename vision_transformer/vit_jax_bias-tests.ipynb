{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"vit_jax_bias-tests.ipynb","provenance":[{"file_id":"https://github.com/google-research/vision_transformer/blob/master/vit_jax.ipynb","timestamp":1605295068581}],"collapsed_sections":["sXhZm0kpPpH6"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-MYot7DJh9kk"},"source":["See code at https://github.com/google-research/vision_transformer/\n","\n","See paper at https://arxiv.org/abs/2010.11929\n","\n","This Colab allows you to run the [JAX](https://jax.readthedocs.org) implementation of the Vision Transformer."]},{"cell_type":"markdown","metadata":{"id":"sXhZm0kpPpH6"},"source":["##### Copyright 2020 Google LLC."]},{"cell_type":"code","metadata":{"id":"KfmzfvFxPuk7","cellView":"both"},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iOVCm4CnP1Do"},"source":["<a href=\"https://colab.research.google.com/github/google-research/vision_transformer/blob/master/vit_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"cyD76dm5JaeW"},"source":["### Setup\n","\n","Needs to be executed once in every VM.\n","\n","The cell below downloads the code from Github and install necessary dependencies."]},{"cell_type":"code","metadata":{"id":"zZvI8OXt78sj","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606042879223,"user_tz":-330,"elapsed":26272,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"08476644503706754908"}},"outputId":"41a07cbd-385f-4c79-d5e3-fba121189db5"},"source":["#@markdown Select whether you would like to store data in your personal drive.\n","#@markdown\n","#@markdown If you select **yes**, you will need to authorize Colab to access\n","#@markdown your personal drive\n","#@markdown\n","#@markdown If you select **no**, then any changes you make will diappear when\n","#@markdown this Colab's VM restarts after some time of inactivity...\n","use_gdrive = 'yes'  #@param [\"yes\", \"no\"]\n","\n","if use_gdrive == 'yes':\n","  from google.colab import drive\n","  drive.mount('/gdrive')\n","  root = '/gdrive/My Drive/Fall 20-21/COS 454/Project/cnn_txf_bias/vision_transformer'\n","  import os\n","  if not os.path.isdir(root):\n","    os.mkdir(root)\n","  os.chdir(root)\n","  print(f'\\nChanged CWD to \"{root}\"')\n","else:\n","  from IPython import display\n","  display.display(display.HTML(\n","      '<h1 style=\"color:red\">CHANGES NOT PERSISTED</h1>'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n","\n","Changed CWD to \"/gdrive/My Drive/Fall 20-21/COS 454/Project/cnn_txf_bias/vision_transformer\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sCN4d-GQJdU4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606042917556,"user_tz":-330,"elapsed":36595,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"08476644503706754908"}},"outputId":"ea144b36-1140-47f5-ca2f-8acbaecd387d"},"source":["!pip install -qr ./vit_jax/requirements.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K     |█████▊                          | 10kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 20kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 30kB 11.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 40kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 51kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.7MB/s \n","\u001b[K     |████████████████████████████████| 153kB 7.5MB/s \n","\u001b[K     |████████████████████████████████| 92kB 5.5MB/s \n","\u001b[K     |████████████████████████████████| 137.3MB 89kB/s \n","\u001b[K     |████████████████████████████████| 4.3MB 48.2MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bcLBTSXuNjK6"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"BcnlJF7FKTfD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605295602244,"user_tz":-330,"elapsed":39842,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}},"outputId":"0b9d5a77-2fe1-4e77-d25b-548749445ad0"},"source":["# Shows all available pre-trained models.\n","!gsutil ls -lh gs://vit_models/*"],"execution_count":null,"outputs":[{"output_type":"stream","text":["      65 B  2020-10-21T07:59:00Z  gs://vit_models/README.txt\n","\n","gs://vit_models/imagenet21k+imagenet2012/:\n","330.29 MiB  2020-10-29T17:05:52Z  gs://vit_models/imagenet21k+imagenet2012/ViT-B_16-224.npz\n"," 331.4 MiB  2020-10-20T11:48:22Z  gs://vit_models/imagenet21k+imagenet2012/ViT-B_16.npz\n","336.89 MiB  2020-10-20T11:47:36Z  gs://vit_models/imagenet21k+imagenet2012/ViT-B_32.npz\n","  1.13 GiB  2020-10-29T17:08:31Z  gs://vit_models/imagenet21k+imagenet2012/ViT-L_16-224.npz\n","  1.14 GiB  2020-10-20T11:53:44Z  gs://vit_models/imagenet21k+imagenet2012/ViT-L_16.npz\n","  1.14 GiB  2020-10-20T11:50:56Z  gs://vit_models/imagenet21k+imagenet2012/ViT-L_32.npz\n","\n","gs://vit_models/imagenet21k/:\n","393.69 MiB  2020-10-22T21:38:39Z  gs://vit_models/imagenet21k/ViT-B_16.npz\n","400.01 MiB  2020-11-02T08:30:56Z  gs://vit_models/imagenet21k/ViT-B_32.npz\n","  2.46 GiB  2020-11-03T10:46:11Z  gs://vit_models/imagenet21k/ViT-H_14.npz\n","  1.22 GiB  2020-11-09T14:39:51Z  gs://vit_models/imagenet21k/ViT-L_16.npz\n","  1.23 GiB  2020-11-02T08:35:10Z  gs://vit_models/imagenet21k/ViT-L_32.npz\n","TOTAL: 12 objects, 10807170099 bytes (10.06 GiB)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6ztOhq_fzZyO"},"source":["# Specify model\n","model = 'ViT-L_32'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4EzOChfJeVrU","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605295194651,"user_tz":-330,"elapsed":23098,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"14014665847910676872"}},"outputId":"3252d5b5-09b4-41f8-9230-ea6808aefa17"},"source":["#@markdown TPU setup : Boilerplate for connecting JAX to TPU.\n","\n","import os\n","# if 'google.colab' in str(get_ipython()) and 'COLAB_TPU_ADDR' in os.environ:\n","#   # Make sure the Colab Runtime is set to Accelerator: TPU.\n","#   import requests\n","#   if 'TPU_DRIVER_MODE' not in globals():\n","#     url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20191206'\n","#     resp = requests.post(url)\n","#     TPU_DRIVER_MODE = 1\n","\n","#   # The following is required to use TPU Driver as JAX's backend.\n","#   from jax.config import config\n","#   config.FLAGS.jax_xla_backend = \"tpu_driver\"\n","#   config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n","#   print('Registered TPU:', config.FLAGS.jax_backend_target)\n","# else:\n","#   print('No TPU detected. Can be changed under \"Runtime/Change runtime type\".')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Registered TPU: grpc://10.70.169.18:8470\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"igqZ6qYNeHWo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606054628299,"user_tz":-330,"elapsed":1028,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"08476644503706754908"}},"outputId":"5e9275cf-bc1a-400a-ffd4-a7967aad067a"},"source":["import flax\n","import jax\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import tqdm\n","\n","# Shows the number of available devices.\n","# In a CPU/GPU runtime this will be a single device.\n","# In a TPU runtime this will be 8 cores.\n","jax.local_devices()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[CpuDevice(id=0)]"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"sjN0_b-YbaHu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606054630063,"user_tz":-330,"elapsed":1043,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"08476644503706754908"}},"outputId":"8b1e6a37-b1f9-4fbc-eff3-67ecee6ce366"},"source":["# Import files from repository.\n","# Updating the files in the editor on the right will immediately update the\n","# modules by re-importing them.\n","\n","import sys\n","if './' not in sys.path:\n","  sys.path.append('./')\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","from vit_jax import checkpoint\n","from vit_jax import hyper\n","from vit_jax import input_pipeline\n","from vit_jax import logging\n","from vit_jax import models\n","from vit_jax import momentum_clip\n","from vit_jax import train\n","\n","logger = logging.setup_logger('./logs')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TE7BoGkyoY7X"},"source":["VisionTransformer = models.KNOWN_MODELS[model].partial(num_classes=1000)\n","\n","# Load and convert pretrained checkpoint.\n","params = checkpoint.load(f'./vit_models/imagenet21k+imagenet2012/{model}.npz')\n","params['pre_logits'] = {}  # Need to restore empty leaf for Flax.\n","\n","# Load and convert fine-tuned model on augmented dataset\n","params = checkpoint.load(f'./vit_models/imagenet21k+imagenet2012+cifar10/ViT-B_32_Baseline+Rotate+Cutout+Sobel Filtering+Gaussian Blur+Color Distortion+Gaussain Noise.npz')\n","params['pre_logits'] = {}  # Need to restore empty leaf for Flax."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p3TTypN0gkwO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606042942968,"user_tz":-330,"elapsed":4841,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"08476644503706754908"}},"outputId":"fb7d6d28-a686-4796-d1d7-97b19e9645a1"},"source":["# Get imagenet labels.\n","!wget https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\n","imagenet_labels = dict(enumerate(open('ilsvrc2012_wordnet_lemmas.txt')))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-11-22 11:02:21--  https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.124.128, 172.217.212.128, 172.217.214.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.124.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 21675 (21K) [text/plain]\n","Saving to: ‘ilsvrc2012_wordnet_lemmas.txt.1’\n","\n","\r          ilsvrc201   0%[                    ]       0  --.-KB/s               \rilsvrc2012_wordnet_ 100%[===================>]  21.17K  --.-KB/s    in 0.002s  \n","\n","2020-11-22 11:02:21 (10.8 MB/s) - ‘ilsvrc2012_wordnet_lemmas.txt.1’ saved [21675/21675]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hwjsOz1bUWn_"},"source":["airplane_indices = [404]\n","bear_indices = [294, 295, 296, 297]\n","bicycle_indices = [444, 671]\n","bird_indices = [8, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 22, 23,\n","                24, 80, 81, 82, 83, 87, 88, 89, 90, 91, 92, 93,\n","                94, 95, 96, 98, 99, 100, 127, 128, 129, 130, 131,\n","                132, 133, 135, 136, 137, 138, 139, 140, 141, 142,\n","                143, 144, 145]\n","boat_indices = [472, 554, 625, 814, 914]\n","bottle_indices = [440, 720, 737, 898, 899, 901, 907]\n","car_indices = [436, 511, 817]\n","cat_indices = [281, 282, 283, 284, 285, 286]\n","chair_indices = [423, 559, 765, 857]\n","clock_indices = [409, 530, 892]\n","dog_indices = [152, 153, 154, 155, 156, 157, 158, 159, 160, 161,\n","                162, 163, 164, 165, 166, 167, 168, 169, 170, 171,\n","                172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n","                182, 183, 184, 185, 186, 187, 188, 189, 190, 191,\n","                193, 194, 195, 196, 197, 198, 199, 200, 201, 202,\n","                203, 205, 206, 207, 208, 209, 210, 211, 212, 213,\n","                214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n","                224, 225, 226, 228, 229, 230, 231, 232, 233, 234,\n","                235, 236, 237, 238, 239, 240, 241, 243, 244, 245,\n","                246, 247, 248, 249, 250, 252, 253, 254, 255, 256,\n","                257, 259, 261, 262, 263, 265, 266, 267, 268]\n","elephant_indices = [385, 386] \n","keyboard_indices = [508, 878]\n","knife_indices = [499]\n","oven_indices = [766]\n","truck_indices = [555, 569, 656, 675, 717, 734, 864, 867]\n","\n","category_indices = [airplane_indices, bear_indices, bicycle_indices, bird_indices, boat_indices,\n","                    bottle_indices, car_indices, cat_indices, chair_indices, clock_indices,\n","                    dog_indices, elephant_indices, keyboard_indices, knife_indices,\n","                    oven_indices, truck_indices]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wrvwNAGJshzb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606060438271,"user_tz":-330,"elapsed":599267,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"08476644503706754908"}},"outputId":"bf08a441-3635-4370-b48e-f0b255c4b8dc"},"source":["import cv2\n","import csv\n","\n","exp = 'silhouettes'\n","categories = os.listdir(\"./stimuli/\" + exp + \"/\")\n","categories.sort()\n","# print(categories)\n","\n","obj_response = []\n","obj_category = []\n","image_name = []\n","\n","print(f'Prediction for model: \"{model}\" on experiment: \"{exp}\"')\n","\n","count = 0\n","for c in categories:\n","  for im in os.listdir(\"./stimuli/\" + exp + \"/\" + c + \"/\"):\n","    image_name.append(im)\n","    img = cv2.imread(\"./stimuli/\" + exp + \"/\" + c + \"/\" + im)\n","    img = cv2.resize(img, (384, 384))\n","    inp = (np.array(img) / 128 - 1)[None, ...]\n","    logits, = VisionTransformer.call(params, inp)\n","    preds = flax.nn.softmax(logits)\n","\n","    preds_16 = np.zeros(16)\n","    for idx in range(1000):\n","      for ci in range(len(category_indices)):\n","        if idx in category_indices[ci]:\n","          preds_16[ci] += preds[idx]\n","\n","    # print(preds_16)\n","    # pred = preds.argsort()[-1]\n","    # print(pred)\n","\n","    obj_category.append(c)\n","    obj_response.append(categories[preds_16.argsort()[-1]])\n","\n","    # obj_resp = ''\n","\n","    # for ci in range(len(category_indices)):\n","    #   if pred in category_indices[ci]:\n","    #     obj_resp = categories[ci]\n","    \n","    # if obj_resp == '':\n","    #   obj_response.append('knife')\n","    # else:\n","    #   obj_response.append(obj_resp)\n","\n","    count = count + 1\n","    print('\\r %0.2f%%' % (count/(16*len(os.listdir(\"./stimuli/\" + exp + \"/\" + c + \"/\")))*100), end='')\n","\n","with open(f'./results/texture-shape_{exp}/texture-shape_{exp}_{model}_session-1.csv', mode='w') as csv_file:\n","    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n","\n","    csv_writer.writerow(['subj', 'session', 'trial', 'rt', 'object_response', 'category', 'condition', 'imagename'])\n","    for e in range(len(obj_response)):\n","      csv_writer.writerow([model, '1', f'{e+1}', 'NaN', obj_response[e], obj_category[e], 'NaN', image_name[e]])\n","\n","# Predict on a batch with a single item (note very efficient TPU usage...)\n","# logits, = VisionTransformer.call(params, (np.array(img) / 128 - 1)[None, ...])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Prediction for model: \"ViT-L_32\" on experiment: \"silhouettes\"\n"," 100.00%"],"name":"stdout"}]}]}